---
title: 'Multidimensional Scaling and Self-Organizing Maps'
author: "Nadia Noui-Mehidi"
date: "10/29/2019"
output:
  word_document: default
  pdf_document: default
subtitle: MSDS 411
---

```{r setup, include=FALSE}

library(wooldridge)
data('recid')

#college <- read.csv(file="file:///Users/nadianoui-mehidi/Desktop/college_acceptance.csv",head=TRUE,sep=",")

#recidivism <- read.csv(file="file:///Users/nadianoui-mehidi/Desktop/recidivism.csv",head=TRUE,sep=",")
library(dplyr)
library(skimr)
library(cluster)
require(tidyverse)
library(kohonen)
require(ggplot2)
require(ggridges)
require(RColorBrewer)
library(ggplot2)
library(vegan)

```

#COMPONENT 1:    Multidimensional Scaling
##1. EDA
The recidivism dataset is an 18 variable dataset with 1445 records.  The data is a random sample of convicts released from prison between July 1, 1977 and June 30, 1978. The information was collected retrospectively by looking at records in April 1984, so the maximum possible length of observation is 81 months. 

The 18 variables including in our data set are:
 1. black =1 if black
 2. alcohol =1 if alcohol problems
 3. drugs =1 if drug history
 4. supervised =1 if release supervised
 5. married =1 if married when incarc.
 6. felony =1 if felony sentence
 7. workprg =1 if in N.C. pris. work prg.
 8. property =1 if property crime
 9. person =1 if crime against person
10. nbr_piors # prior convictions
11. education years of schooling
12. nbr_rules # rules violations in prison
13. age in months
14. time_served time served, rounded to months
15. follow_up length follow period, months
16. duration max(time until return, follow)
17. censored =1 if duration right censored
18. log_duration log(duration)



```{r}
glimpse(recid)
```
There are 9 binary, 8 interval and 1 continuous variable in our dataset. We normally dont want to include categorical variables when multidemenisional scaling but since our categorical variables are binary their distance measures are still meaningful and we can keep them in. 

Of interest to us is the time until they return to prison (duration variable), so with this in mind, we will keep the following variables:  
duration:months until return to prison
censored:censoring indicator variable
workprg: indicator of participation in a work program
nbr_priors: the number of previous convictions 
time_served: the time served rounded to months 
felony: an indicator of felony sentences
alcohol: an indicator of alcohol problems
drugs: an indicator of drug use history
black: an indicator for African Americans 
married: an indicator if married when incarcerated 
education: the number of years of schooling 
age: in months.

We created a new dataframe with only the variables we are using. Since our variables have different scales, we also standardize the records so that all variables have an equal opportunity to influence the patterns.

```{r}
df = subset(recid, select = -c(super, property, person, rules, follow, ldurat))
```


##2. Dissimilarity Matrix using Euclidean Distances. 
The input data for MDS is a dissimilarity matrix representing the distances between pairs of objects. We measure distance as the Euclidean Distance between two points. 
Our dissimilarity matrix output doesn't look right, the numbers increases by 1 until it reaches the total number of records. It's not symmetrical and doesnt ressemble a distance matrix, I think that the code is only showing the first column in the matrix.  When we print a summary of the matrix it looks like it did run correctly and is just unable to print the entire matrix so we will proceed with the analysis. 


```{r}
d<- daisy(df, metric = "euclidean")
#d
print(d)
summary(d)
```


##3. Classical Multidimensional Scaling   
 
### Fit 
We use our dissimilarity matrix to create a classical multidemensional scaling model and then assess the goodness of fit using a Shepard diagram and our stress value. 
A Shepard diagram compares how far apart our data points are before and after we transform them as a scatter plot. The diagram below shows the original distance between points on the y-axis and the new MDS ordination distance on the x-axis. The vertical line from each point to the regression function gives the residual of that particular point. Our Shepard diagram suggest that our model is a pretty good fit. 
The stress value is another indicator of whether the patterns present in the original distance matrix are adequately reproduced from the 2 new dimensions. The smaller the stress value, the better the match between data and ordination distances and thus the better the new two-dimensional configuration represents the patterns based on all the original variables. As we can see in the results, our stress value is 0.02809177. A stress value below 0.05 is considered an excellent representation in reduced dimensions.
```{r}
d1 <- dist(df) # euclidean distances between the rows
#fit <- cmdscale(d1, eig=TRUE, k=2) # k is the number of dim
fit <- metaMDS(d1, k=2)
fit # view results
stressplot(fit)

```

### MDS Plot
We have plotted our points on a 2-D coordinate system using MDS. The points are arranged so that the distances among each pair of points correlates as best as possible to the dissimilarity between those two samples. We have color coded each point by its duration value to see if we can find any meaningful patters corresponding to time until an individual returns to prison. 
```{r}
# plot solution
Dimension1 <- fit$points[,1]
Dimension2 <- fit$points[,2]
MDS_xy <- data.frame(fit$points)
MDS_xy$durat <- df$durat

ggplot(MDS_xy, aes(Dimension1, Dimension2, color = durat)) + geom_point() + theme_bw()
```

Our plot shows clear groupings of samples that correspond with duration lengths; records are likely to be similar to other records of the same duration. This means we can potentially predict whether a prisoner is likely to return to jail based on our other variables.


##4. Non-Metric scaling.
In our previous multidimensional scaling model we tried to preserve the distance between points. This time we will use non-metric multidimensional scaling where it’s not the distance between points that is important but rather its rank. This model is more suitable for qualitative data. 

###Fit
Our Stress value is 0.003198809 which again indicates a very strong fit and tells us that we will be faithfully representing our data in two dimensions. Our stress value is a lot smaller than it was using the classical approach. It makes sense for the non-metric model to have a better fit since we have mostly qualitative variables. 

```{r}
library(MASS)
d2 <- dist(df) # euclidean distances between the rows
#fit <- isoMDS(d2, k=2) # k is the number of dim
fit <- sammon(d2)
fit # view results

```


###MDS Plot
Since we had a strong goodness of fit for both models, it makes sense that the graphical representaiton of the data is similar for both models. Our plot has grouped our data identically to the classical model, we have same two clusters of samples that correspond with duration lengths.

```{r}
# plot solution
Dimension1 <- fit$points[,1]
Dimension2 <- fit$points[,2]
MDS_xy <- data.frame(fit$points)
MDS_xy$durat <- df$durat
ggplot(MDS_xy, aes(Dimension1, Dimension2, color = durat)) + geom_point() + theme_bw()
```
##Ramsey’s method


# COMPONENT 2: Self Organizing Maps
We are using the College Acceptance data set to create a Self Organizing Map. This dataset contains information for college acceptance into various engineering programs for 400 students.  

## Exploratory Data Analysis and Data Preparation 
This data set represents college applicants. 
The variables included are:
•	admit (binary)  0 = Not admitted, 1 = Admitted
•	gre (numeric)  Student’s GRE score
•	gpa (numeric)  Student’s GPA
•	rank (numeric)  College ranking

Since the GRE/GPA/Rank are explanatory variables for Admit, we will eliminate the Admit variable from the dataset and focus our SOM on GRE, GPA, and Rank. We have also normalized the data because the variables have different scales and we dont want GRE to have outsized influence in our model. The summary of our data after normalization is below:

```{r}
college = read.csv('https://stats.idre.ucla.edu/stat/data/binary.csv')
#print(college)
#college <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
X <- scale(college)
X <- scale(college[,-1])
summary(X)
```

##6. The SOM model.   

The SOM is made up of several nodes and input records are mapped to the most similar node. We used the ‘find_grid_size’ function to calculate the suggested size of our map, it used the count of observations in our dataset to determine the best grid size was 100 nodes.This grid may end up being too large to represent the number of different unique groups students fall into but we can adjust once we see how the records are assigned to the nodes. We used 100 epochs to train the model.
The 100 nodes produced by our SOM and their corresponding weights are below: 
```{r}
#map_dimension = find_grid_size(dim(X)[1])

```


```{r}
set.seed(222)
g <- somgrid(xdim = 10, ydim = 10, topo = "rectangular" )
map <- som(X,
           grid = g,
           alpha = c(0.05, 0.01),
           radius = 1)
map$codes
```


##7. Evaluate the SOM model.   

As the SOM training iterations progress, the distance from each node’s weights to the samples represented by that node is reduced. Ideally, this distance should reach a minimum plateau. The training plot below shows our models progress over time. We can see that the grid’s shape stabilizes really quickly after a couple iterations. We did not need as many iterations as we used. 
```{r}
plot(map, type="changes")
```

###Counts Plot
To see how our data points are distributed among the nodes we produce a count and map plot. This metric can be used to evaluate our grid size – ideally we want our points to be uniformly distributed among the nodes. Large values in some nodes suggests that a larger grid would be benificial. Empty nodes indicate that our grid is too big for the number of samples. We are aiming for at least 5-10 samples per node. We have a lot of one to one relationships between nodes and records, it looks like our grid is way too big.

```{r}
plot(map,
     type='count')
plot(map,
     type='mapping',
     palette.name=rainbow)
```

### Distance Map 
The distance map is a visualization of the distance between each node and its neighbours. Dark colors depict closely spaced node codebook vectors and lighter colors indicate more widely separated node codebook vectors. Groups of dark colors can be considered as clusters, and the light parts as the boundaries between the clusters. This representation can be used to identify clusters within the SOM map. Our distance plot doesnt show any clear cluster patterns in our data. 

```{r}
plot(map,
     type='dist.neighbours')

```
###Codes Plot
The fan diagram shows the distribution of variables across the map. We can see patterns by examing dominant colours and ratios between colors. In our codes plot there are a lot of duplicate fan patterns or very similar fan patterns. We need a smaller grid space. 
```{r}
plot(map)
```

##8.	Experiment with the SOM model. 
This time we recreate our SOM model with a much smaller grid. We are using a 4 x 4 rectangular grid, with 16 neurons in total. 
```{r}
set.seed(222)
f <- somgrid(xdim = 4, ydim = 4, topo = "rectangular" )
mapp <- som(X,
           grid = f,
           alpha = c(0.05, 0.01),
           radius = 1)

```
##Counts Plot
The smaller grid plot is an improvement over our large map space. Our records are much more evenly distributed among the nodes.  
```{r}
plot(mapp,
     type='count')
plot(mapp,
     type='mapping')
```


###Distance Map
The distance map is a visualization of the distance between each node and its neighbours. It can be used to identify clusters within the SOM map. Our distance plot still doesnt show any clear cluster patterns in our data. 

```{r}
plot(mapp,
     type='dist.neighbours')

```
### Codes Plot 
The fan diagram shows the distribution of variables across the map. We can see patterns by examing dominant colours and ratios between colors. 
Using map$unit.classif we see that the second record in our dataset was categorized into node 3, the node in the last row and 3rd column of our nodes plot. The fan sizes tell us that applications within this node have high GPA values but low GRE values and the rank of the institution is somewhere in the middle (2nd or 3rd rank). When we look at the second record in our dataframe we see the GRE is 660, gpa is 3.67 and rank is 3.
The third record has a GRE of 800, a GPA of 4.00 and a rank of 1, it was categorized into the 12th node (2nd row, last column). This kind of applicant is more likely to be accepted.
It looks like we have a better variety of different application types. Eache node looks like a unique mix of GRE, GPA and rank.

```{r}
plot(mapp)
mapp$unit.classif
head(college)
```

##9. Reflection

I found MDS to be an easier approach for visualization of multidimensional data than SOM Multidimensional scaling provides a well defined measure of the quality of maps a measure that may also be used to compare the quality of different Kohonen maps It also was easier to visualize. I found the codes plot a bit confusing and wasnt able to gain any insight from the distance maps.



